{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajmGLSCo2hiW",
        "outputId": "50da00fb-8ce9-4782-f86b-55b7ff1f291d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PMLDL_Assignment1'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 189 (delta 64), reused 149 (delta 44), pack-reused 18\u001b[K\n",
            "Receiving objects: 100% (189/189), 42.95 MiB | 33.73 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "/content/PMLDL_Assignment1/notebooks\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  !git clone https://github.com/system205/PMLDL_Assignment1.git\n",
        "  %cd /content/PMLDL_Assignment1/notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "root='../'\n",
        "project_path='/workspaces/PMLDL_Assignment1'\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    project_path = '/content/PMLDL_Assignment1'\n",
        "\n",
        "\n",
        "!pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "577760\n"
          ]
        }
      ],
      "source": [
        "!python ../src/data/preprocess_dataset.py --root={root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter and split on train and validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python ../src/models/filter_split_train_val.py --root={root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test simple models - hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test baseline model to compare all other metrics with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-29 13:29:51.028248: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-29 13:29:51.028308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-29 13:29:51.032495: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-29 13:29:52.125568: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspaces/PMLDL_Assignment1/notebooks/../src/models/metric/compute_metric.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataframe = pd.read_csv(dataframe_file, sep=separator)\n",
            "\n",
            "(Translation (target), Prediction (output), Reference (input))\n",
            "(\"you didn't know that Estelle stole your fish from the garbage.\", \"You didn't know that Estelle had stolen some fish from your bin.\", \"You didn't know that Estelle had stolen some fish from your bin.\")\n",
            "(\"you'd be sucked out of your life!\", \"It'il suck the life out of you!\", \"It'il suck the life out of you!\")\n",
            "(\"I really can't take this.\", \"I can't fuckin' take that, bruv.\", \"I can't fuckin' take that, bruv.\")\n",
            "(\"they said I was a hero, but I didn't care.\", \"They called me a fucking hero. The truth is I didn't care anymore.\", \"They called me a fucking hero. The truth is I didn't care anymore.\")\n",
            "(\"I didn't fuck him.\", 'I did not screw him.', 'I did not screw him.')\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "Computed sacrebleu: {'score': 22.8917754786834, 'counts': [3908067, 1973217, 1078158, 580476], 'totals': [7372539, 6794762, 6217078, 5642973], 'precisions': [53.0084276258152, 29.040266605364543, 17.341876682261347, 10.286705252709874], 'bp': 1.0, 'sys_len': 7372539, 'ref_len': 6899727}\n",
            "Score: 22.89%\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/metric/compute_metric.py --dataframe_file={root}data/raw/filtered.tsv --predictions_column=reference --separator='\\t'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-23 18:32:29.174844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-23 18:32:29.174898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-23 18:32:29.174960: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-23 18:32:29.181973: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspaces/PMLDL_Assignment1/notebooks/../src/models/metric/compute_metric.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
            "  dataframe = pd.read_csv(dataframe_file, sep=separator)\n",
            "\n",
            "(Translation, Prediction)\n",
            "('you didn t know that estelle had stolen some fish from your bin', 'you didn t know that estelle stole your fish from the garbage')\n",
            "('you d be sucked out of your life', 'it il suck the life out of you')\n",
            "('i really can t take this', 'i can t fuckin take that bruv')\n",
            "('they said i was a hero but i didn t care', 'they called me a fucking hero the truth is i didn t care anymore')\n",
            "('i did not screw him', 'i didn t fuck him')\n",
            "Computed sacrebleu: {'score': 25.21085920534741, 'counts': [3456677, 1774830, 956137, 515615], 'totals': [6130128, 5552368, 4978722, 4418196], 'precisions': [56.388333163679455, 31.96528039928189, 19.204466527755518, 11.670260893812769], 'bp': 1.0, 'sys_len': 6130128, 'ref_len': 5983432}\n",
            "Score: 25.21%\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/metric/compute_metric.py --dataframe_file={root}data/interim/preprocessed.csv --predictions_column=reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-29 13:41:38.219448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-29 13:41:38.219505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-29 13:41:38.219548: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-29 13:41:38.226283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspaces/PMLDL_Assignment1/notebooks/../src/models/metric/compute_metric.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
            "  dataframe = pd.read_csv(dataframe_file, sep=separator)\n",
            "\n",
            "(Translation (target), Prediction (output), Reference (input))\n",
            "('it s so liberating', 'that is fucking liberating', 'that is fucking liberating')\n",
            "('henry you ve had a really tough year', 'henry you have had a damn tough year', 'henry you have had a damn tough year')\n",
            "('he s undecided as his father at his age', 'he s as imprudent as his father was at his age', 'he s as imprudent as his father was at his age')\n",
            "('i can do what i want', 'i can do whatever the fuck i want', 'i can do whatever the fuck i want')\n",
            "('what s going on dad', 'what the hell dad', 'what the hell dad')\n",
            "Computed sacrebleu: {'score': 37.47398075879449, 'counts': [152744, 87920, 50781, 27722], 'totals': [226254, 194741, 163505, 133068], 'precisions': [67.50996667462233, 45.147144155570736, 31.057765817559098, 20.832957585595334], 'bp': 1.0, 'sys_len': 226254, 'ref_len': 205842}\n",
            "Score: 37.47%\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/metric/compute_metric.py --dataframe_file={root}data/interim/train.csv --predictions_column=reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-29 13:42:05.405516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-29 13:42:05.405581: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-29 13:42:05.405639: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-29 13:42:05.412453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspaces/PMLDL_Assignment1/notebooks/../src/models/metric/compute_metric.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
            "  dataframe = pd.read_csv(dataframe_file, sep=separator)\n",
            "\n",
            "(Translation (target), Prediction (output), Reference (input))\n",
            "('if you re not there you re fired', 'you re not out there you re fucking fired', 'you re not out there you re fucking fired')\n",
            "('vee you know we won t say anything', 'vee you know we ain t gonna say shit', 'vee you know we ain t gonna say shit')\n",
            "('not for long', 'not for fucking long', 'not for fucking long')\n",
            "('i thought so but why do i feel like it s me', 'i thought that but why do i feel like the bitch was me', 'i thought that but why do i feel like the bitch was me')\n",
            "('it s like it s always been this million in three hours', 'as always the point is this million in three fucking hours', 'as always the point is this million in three fucking hours')\n",
            "Computed sacrebleu: {'score': 37.65266921709582, 'counts': [38232, 22003, 12751, 6987], 'totals': [56520, 48641, 40834, 33215], 'precisions': [67.64331210191082, 45.23550091486606, 31.226428956261937, 21.035676652115008], 'bp': 1.0, 'sys_len': 56520, 'ref_len': 51447}\n",
            "Score: 37.65%\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/metric/compute_metric.py --dataframe_file={root}data/interim/val.csv --predictions_column=reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Result\n",
        "\n",
        "### As you can see the highest score (37%) is on the filtered data: train or val datasets. So, the final solution will be compared with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test hypothesis 1 model (simple removal of swearing words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of toxic words to be filtered: 2858\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/train_simple_model.py --project_path={project_path} --root={root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate on validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of bad words to be filtered: 2858\n",
            "2023-10-29 13:42:18.300705: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-29 13:42:18.300770: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-29 13:42:18.300829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-29 13:42:18.307669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspaces/PMLDL_Assignment1/notebooks/../src/models/metric/compute_metric.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
            "  dataframe = pd.read_csv(dataframe_file, sep=separator)\n",
            "\n",
            "(Translation (target), Prediction (output), Reference (input))\n",
            "('if you re not there you re fired', 'you re not out there you re fired ', 'you re not out there you re fucking fired')\n",
            "('vee you know we won t say anything', 'vee you know we ain t gonna say ', 'vee you know we ain t gonna say shit')\n",
            "('not for long', 'not for long ', 'not for fucking long')\n",
            "('i thought so but why do i feel like it s me', 'i thought that but why do i feel like the was me ', 'i thought that but why do i feel like the bitch was me')\n",
            "('it s like it s always been this million in three hours', 'as always the point is this million in three hours ', 'as always the point is this million in three fucking hours')\n",
            "Computed sacrebleu: {'score': 45.97528123119192, 'counts': [37987, 23195, 14024, 7995], 'totals': [49967, 42088, 34429, 27127], 'precisions': [76.02417595613105, 55.1107203953621, 40.73310290743269, 29.472481291701996], 'bp': 0.9708148108585534, 'sys_len': 49967, 'ref_len': 51447}\n",
            "Score: 45.98%\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/predict_simple_model.py --dataframe_file=../data/interim/val.csv --root={root}\n",
        "!python ../src/models/metric/compute_metric.py --dataframe_file=../data/interim/val.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The result of simple solution is quite good. It is 8% more than of baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final model - T5 fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-30 14:41:24.754229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-30 14:41:24.754283: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-30 14:41:24.754324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-30 14:41:28.323562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)okenizer_config.json: 100% 2.32k/2.32k [00:00<00:00, 11.2MB/s]\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 6.13MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 40.3MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 6.68MB/s]\n",
            "Downloading model.safetensors: 100% 242M/242M [00:09<00:00, 24.3MB/s]\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 880kB/s]\n",
            "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 25.7MB/s]\n",
            "Map: 100% 31513/31513 [00:04<00:00, 6457.59 examples/s]\n",
            "Map: 100% 7879/7879 [00:00<00:00, 12947.57 examples/s]\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.7956, 'learning_rate': 1.9887196841511564e-05, 'epoch': 0.25}\n",
            "{'loss': 1.4945, 'learning_rate': 1.9774393683023126e-05, 'epoch': 0.51}\n",
            "{'loss': 1.3968, 'learning_rate': 1.9661590524534688e-05, 'epoch': 0.76}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.2162691354751587, 'eval_bleu': 48.0982, 'eval_gen_len': 9.2191, 'eval_runtime': 95.9245, 'eval_samples_per_second': 82.137, 'eval_steps_per_second': 5.139, 'epoch': 1.0}\n",
            "{'loss': 1.3815, 'learning_rate': 1.954878736604625e-05, 'epoch': 1.02}\n",
            "{'loss': 1.3297, 'learning_rate': 1.9435984207557815e-05, 'epoch': 1.27}\n",
            "{'loss': 1.3021, 'learning_rate': 1.9323181049069374e-05, 'epoch': 1.52}\n",
            "{'loss': 1.2773, 'learning_rate': 1.921037789058094e-05, 'epoch': 1.78}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.1516704559326172, 'eval_bleu': 49.1652, 'eval_gen_len': 9.1672, 'eval_runtime': 92.762, 'eval_samples_per_second': 84.938, 'eval_steps_per_second': 5.315, 'epoch': 2.0}\n",
            "{'loss': 1.2691, 'learning_rate': 1.90975747320925e-05, 'epoch': 2.03}\n",
            "{'loss': 1.2404, 'learning_rate': 1.8984771573604063e-05, 'epoch': 2.28}\n",
            "{'loss': 1.2504, 'learning_rate': 1.8871968415115625e-05, 'epoch': 2.54}\n",
            "{'loss': 1.2272, 'learning_rate': 1.8759165256627187e-05, 'epoch': 2.79}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.116970419883728, 'eval_bleu': 49.9642, 'eval_gen_len': 9.1466, 'eval_runtime': 93.5211, 'eval_samples_per_second': 84.248, 'eval_steps_per_second': 5.272, 'epoch': 3.0}\n",
            "{'loss': 1.1998, 'learning_rate': 1.864636209813875e-05, 'epoch': 3.05}\n",
            "{'loss': 1.2029, 'learning_rate': 1.853355893965031e-05, 'epoch': 3.3}\n",
            "{'loss': 1.1795, 'learning_rate': 1.8420755781161873e-05, 'epoch': 3.55}\n",
            "{'loss': 1.1863, 'learning_rate': 1.830795262267344e-05, 'epoch': 3.81}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.090863585472107, 'eval_bleu': 50.5527, 'eval_gen_len': 9.1522, 'eval_runtime': 92.1831, 'eval_samples_per_second': 85.471, 'eval_steps_per_second': 5.348, 'epoch': 4.0}\n",
            "{'loss': 1.1943, 'learning_rate': 1.8195149464184997e-05, 'epoch': 4.06}\n",
            "{'loss': 1.1722, 'learning_rate': 1.808234630569656e-05, 'epoch': 4.31}\n",
            "{'loss': 1.16, 'learning_rate': 1.7969543147208125e-05, 'epoch': 4.57}\n",
            "{'loss': 1.1449, 'learning_rate': 1.7856739988719684e-05, 'epoch': 4.82}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0752805471420288, 'eval_bleu': 50.902, 'eval_gen_len': 9.1456, 'eval_runtime': 92.7141, 'eval_samples_per_second': 84.982, 'eval_steps_per_second': 5.317, 'epoch': 5.0}\n",
            "{'loss': 1.1412, 'learning_rate': 1.774393683023125e-05, 'epoch': 5.08}\n",
            "{'loss': 1.1413, 'learning_rate': 1.763113367174281e-05, 'epoch': 5.33}\n",
            "{'loss': 1.1297, 'learning_rate': 1.7518330513254373e-05, 'epoch': 5.58}\n",
            "{'loss': 1.1198, 'learning_rate': 1.7405527354765935e-05, 'epoch': 5.84}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0634514093399048, 'eval_bleu': 51.0776, 'eval_gen_len': 9.1349, 'eval_runtime': 92.413, 'eval_samples_per_second': 85.259, 'eval_steps_per_second': 5.335, 'epoch': 6.0}\n",
            "{'loss': 1.1223, 'learning_rate': 1.7292724196277497e-05, 'epoch': 6.09}\n",
            "{'loss': 1.1207, 'learning_rate': 1.717992103778906e-05, 'epoch': 6.35}\n",
            "{'loss': 1.1011, 'learning_rate': 1.706711787930062e-05, 'epoch': 6.6}\n",
            "{'loss': 1.106, 'learning_rate': 1.6954314720812183e-05, 'epoch': 6.85}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.053760051727295, 'eval_bleu': 51.3303, 'eval_gen_len': 9.1698, 'eval_runtime': 95.9983, 'eval_samples_per_second': 82.074, 'eval_steps_per_second': 5.136, 'epoch': 7.0}\n",
            "{'loss': 1.0907, 'learning_rate': 1.684151156232375e-05, 'epoch': 7.11}\n",
            "{'loss': 1.1051, 'learning_rate': 1.672870840383531e-05, 'epoch': 7.36}\n",
            "{'loss': 1.0868, 'learning_rate': 1.661590524534687e-05, 'epoch': 7.61}\n",
            "{'loss': 1.0801, 'learning_rate': 1.6503102086858435e-05, 'epoch': 7.87}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0407402515411377, 'eval_bleu': 51.4024, 'eval_gen_len': 9.185, 'eval_runtime': 95.3414, 'eval_samples_per_second': 82.64, 'eval_steps_per_second': 5.171, 'epoch': 8.0}\n",
            "{'loss': 1.0825, 'learning_rate': 1.6390298928369997e-05, 'epoch': 8.12}\n",
            "{'loss': 1.0632, 'learning_rate': 1.627749576988156e-05, 'epoch': 8.38}\n",
            "{'loss': 1.0653, 'learning_rate': 1.616469261139312e-05, 'epoch': 8.63}\n",
            "{'loss': 1.0736, 'learning_rate': 1.6051889452904683e-05, 'epoch': 8.88}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0364161729812622, 'eval_bleu': 51.7024, 'eval_gen_len': 9.1609, 'eval_runtime': 93.8639, 'eval_samples_per_second': 83.941, 'eval_steps_per_second': 5.252, 'epoch': 9.0}\n",
            "{'loss': 1.0578, 'learning_rate': 1.5939086294416245e-05, 'epoch': 9.14}\n",
            "{'loss': 1.0524, 'learning_rate': 1.5826283135927807e-05, 'epoch': 9.39}\n",
            "{'loss': 1.0493, 'learning_rate': 1.571347997743937e-05, 'epoch': 9.64}\n",
            "{'loss': 1.0564, 'learning_rate': 1.5600676818950934e-05, 'epoch': 9.9}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0286699533462524, 'eval_bleu': 51.7282, 'eval_gen_len': 9.147, 'eval_runtime': 93.5872, 'eval_samples_per_second': 84.189, 'eval_steps_per_second': 5.268, 'epoch': 10.0}\n",
            "{'loss': 1.0427, 'learning_rate': 1.5487873660462493e-05, 'epoch': 10.15}\n",
            "{'loss': 1.0316, 'learning_rate': 1.5375070501974055e-05, 'epoch': 10.41}\n",
            "{'loss': 1.0504, 'learning_rate': 1.526226734348562e-05, 'epoch': 10.66}\n",
            "{'loss': 1.0358, 'learning_rate': 1.514946418499718e-05, 'epoch': 10.91}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.022443413734436, 'eval_bleu': 51.8377, 'eval_gen_len': 9.1505, 'eval_runtime': 93.497, 'eval_samples_per_second': 84.27, 'eval_steps_per_second': 5.273, 'epoch': 11.0}\n",
            "{'loss': 1.024, 'learning_rate': 1.5036661026508743e-05, 'epoch': 11.17}\n",
            "{'loss': 1.0311, 'learning_rate': 1.4923857868020306e-05, 'epoch': 11.42}\n",
            "{'loss': 1.0283, 'learning_rate': 1.4811054709531868e-05, 'epoch': 11.68}\n",
            "{'loss': 1.0295, 'learning_rate': 1.469825155104343e-05, 'epoch': 11.93}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0164215564727783, 'eval_bleu': 51.9384, 'eval_gen_len': 9.1909, 'eval_runtime': 94.5298, 'eval_samples_per_second': 83.349, 'eval_steps_per_second': 5.215, 'epoch': 12.0}\n",
            "{'loss': 1.0193, 'learning_rate': 1.4585448392554992e-05, 'epoch': 12.18}\n",
            "{'loss': 1.0145, 'learning_rate': 1.4472645234066556e-05, 'epoch': 12.44}\n",
            "{'loss': 1.0129, 'learning_rate': 1.4359842075578116e-05, 'epoch': 12.69}\n",
            "{'loss': 1.0059, 'learning_rate': 1.424703891708968e-05, 'epoch': 12.94}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0139051675796509, 'eval_bleu': 51.9795, 'eval_gen_len': 9.1335, 'eval_runtime': 93.9963, 'eval_samples_per_second': 83.822, 'eval_steps_per_second': 5.245, 'epoch': 13.0}\n",
            "{'loss': 1.0104, 'learning_rate': 1.4134235758601242e-05, 'epoch': 13.2}\n",
            "{'loss': 0.9952, 'learning_rate': 1.4021432600112806e-05, 'epoch': 13.45}\n",
            "{'loss': 0.997, 'learning_rate': 1.3908629441624366e-05, 'epoch': 13.71}\n",
            "{'loss': 1.0141, 'learning_rate': 1.3795826283135928e-05, 'epoch': 13.96}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0093995332717896, 'eval_bleu': 52.1435, 'eval_gen_len': 9.16, 'eval_runtime': 93.3809, 'eval_samples_per_second': 84.375, 'eval_steps_per_second': 5.279, 'epoch': 14.0}\n",
            "{'loss': 1.0029, 'learning_rate': 1.3683023124647492e-05, 'epoch': 14.21}\n",
            "{'loss': 1.0014, 'learning_rate': 1.3570219966159052e-05, 'epoch': 14.47}\n",
            "{'loss': 0.9762, 'learning_rate': 1.3457416807670616e-05, 'epoch': 14.72}\n",
            "{'loss': 0.998, 'learning_rate': 1.3344613649182178e-05, 'epoch': 14.97}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0070797204971313, 'eval_bleu': 52.2244, 'eval_gen_len': 9.1533, 'eval_runtime': 94.7526, 'eval_samples_per_second': 83.153, 'eval_steps_per_second': 5.203, 'epoch': 15.0}\n",
            "{'loss': 0.9765, 'learning_rate': 1.323181049069374e-05, 'epoch': 15.23}\n",
            "{'loss': 0.9694, 'learning_rate': 1.3119007332205302e-05, 'epoch': 15.48}\n",
            "{'loss': 0.9946, 'learning_rate': 1.3006204173716866e-05, 'epoch': 15.74}\n",
            "{'loss': 0.987, 'learning_rate': 1.2893401015228428e-05, 'epoch': 15.99}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0026341676712036, 'eval_bleu': 52.2752, 'eval_gen_len': 9.1528, 'eval_runtime': 94.8279, 'eval_samples_per_second': 83.087, 'eval_steps_per_second': 5.199, 'epoch': 16.0}\n",
            "{'loss': 0.9803, 'learning_rate': 1.278059785673999e-05, 'epoch': 16.24}\n",
            "{'loss': 0.9896, 'learning_rate': 1.2667794698251552e-05, 'epoch': 16.5}\n",
            "{'loss': 0.9538, 'learning_rate': 1.2554991539763116e-05, 'epoch': 16.75}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 1.0020047426223755, 'eval_bleu': 52.3447, 'eval_gen_len': 9.1151, 'eval_runtime': 94.3346, 'eval_samples_per_second': 83.522, 'eval_steps_per_second': 5.226, 'epoch': 17.0}\n",
            "{'loss': 0.972, 'learning_rate': 1.2442188381274676e-05, 'epoch': 17.01}\n",
            "{'loss': 0.9711, 'learning_rate': 1.2329385222786238e-05, 'epoch': 17.26}\n",
            "{'loss': 0.9577, 'learning_rate': 1.2216582064297802e-05, 'epoch': 17.51}\n",
            "{'loss': 0.9661, 'learning_rate': 1.2103778905809365e-05, 'epoch': 17.77}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9983801245689392, 'eval_bleu': 52.3207, 'eval_gen_len': 9.1553, 'eval_runtime': 93.7642, 'eval_samples_per_second': 84.03, 'eval_steps_per_second': 5.258, 'epoch': 18.0}\n",
            "{'loss': 0.9593, 'learning_rate': 1.1990975747320926e-05, 'epoch': 18.02}\n",
            "{'loss': 0.959, 'learning_rate': 1.1878172588832488e-05, 'epoch': 18.27}\n",
            "{'loss': 0.9581, 'learning_rate': 1.1765369430344052e-05, 'epoch': 18.53}\n",
            "{'loss': 0.9578, 'learning_rate': 1.1652566271855612e-05, 'epoch': 18.78}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9964120388031006, 'eval_bleu': 52.3684, 'eval_gen_len': 9.1433, 'eval_runtime': 92.6951, 'eval_samples_per_second': 84.999, 'eval_steps_per_second': 5.319, 'epoch': 19.0}\n",
            "{'loss': 0.9516, 'learning_rate': 1.1539763113367176e-05, 'epoch': 19.04}\n",
            "{'loss': 0.9348, 'learning_rate': 1.1426959954878738e-05, 'epoch': 19.29}\n",
            "{'loss': 0.971, 'learning_rate': 1.1314156796390301e-05, 'epoch': 19.54}\n",
            "{'loss': 0.9368, 'learning_rate': 1.1201353637901862e-05, 'epoch': 19.8}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9941133856773376, 'eval_bleu': 52.4519, 'eval_gen_len': 9.1425, 'eval_runtime': 93.6573, 'eval_samples_per_second': 84.126, 'eval_steps_per_second': 5.264, 'epoch': 20.0}\n",
            "{'loss': 0.959, 'learning_rate': 1.1088550479413425e-05, 'epoch': 20.05}\n",
            "{'loss': 0.9355, 'learning_rate': 1.0975747320924987e-05, 'epoch': 20.3}\n",
            "{'loss': 0.9594, 'learning_rate': 1.0862944162436548e-05, 'epoch': 20.56}\n",
            "{'loss': 0.9399, 'learning_rate': 1.0750141003948111e-05, 'epoch': 20.81}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9911520481109619, 'eval_bleu': 52.5422, 'eval_gen_len': 9.1295, 'eval_runtime': 93.7005, 'eval_samples_per_second': 84.087, 'eval_steps_per_second': 5.261, 'epoch': 21.0}\n",
            "{'loss': 0.9464, 'learning_rate': 1.0637337845459674e-05, 'epoch': 21.07}\n",
            "{'loss': 0.9395, 'learning_rate': 1.0524534686971236e-05, 'epoch': 21.32}\n",
            "{'loss': 0.9305, 'learning_rate': 1.0411731528482798e-05, 'epoch': 21.57}\n",
            "{'loss': 0.9375, 'learning_rate': 1.0298928369994361e-05, 'epoch': 21.83}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9922555685043335, 'eval_bleu': 52.5726, 'eval_gen_len': 9.1438, 'eval_runtime': 95.0985, 'eval_samples_per_second': 82.851, 'eval_steps_per_second': 5.184, 'epoch': 22.0}\n",
            "{'loss': 0.9414, 'learning_rate': 1.0186125211505923e-05, 'epoch': 22.08}\n",
            "{'loss': 0.9327, 'learning_rate': 1.0073322053017485e-05, 'epoch': 22.34}\n",
            "{'loss': 0.9251, 'learning_rate': 9.960518894529047e-06, 'epoch': 22.59}\n",
            "{'loss': 0.9335, 'learning_rate': 9.84771573604061e-06, 'epoch': 22.84}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9910937547683716, 'eval_bleu': 52.6539, 'eval_gen_len': 9.1248, 'eval_runtime': 93.9897, 'eval_samples_per_second': 83.828, 'eval_steps_per_second': 5.245, 'epoch': 23.0}\n",
            "{'loss': 0.927, 'learning_rate': 9.734912577552173e-06, 'epoch': 23.1}\n",
            "{'loss': 0.9173, 'learning_rate': 9.622109419063735e-06, 'epoch': 23.35}\n",
            "{'loss': 0.9135, 'learning_rate': 9.509306260575297e-06, 'epoch': 23.6}\n",
            "{'loss': 0.9301, 'learning_rate': 9.39650310208686e-06, 'epoch': 23.86}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9860405921936035, 'eval_bleu': 52.5793, 'eval_gen_len': 9.143, 'eval_runtime': 94.0392, 'eval_samples_per_second': 83.784, 'eval_steps_per_second': 5.242, 'epoch': 24.0}\n",
            "{'loss': 0.9229, 'learning_rate': 9.283699943598421e-06, 'epoch': 24.11}\n",
            "{'loss': 0.9149, 'learning_rate': 9.170896785109983e-06, 'epoch': 24.37}\n",
            "{'loss': 0.9246, 'learning_rate': 9.058093626621545e-06, 'epoch': 24.62}\n",
            "{'loss': 0.9223, 'learning_rate': 8.945290468133109e-06, 'epoch': 24.87}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9882665276527405, 'eval_bleu': 52.6913, 'eval_gen_len': 9.1338, 'eval_runtime': 93.3061, 'eval_samples_per_second': 84.443, 'eval_steps_per_second': 5.284, 'epoch': 25.0}\n",
            "{'loss': 0.9213, 'learning_rate': 8.832487309644671e-06, 'epoch': 25.13}\n",
            "{'loss': 0.9144, 'learning_rate': 8.719684151156233e-06, 'epoch': 25.38}\n",
            "{'loss': 0.9125, 'learning_rate': 8.606880992667795e-06, 'epoch': 25.63}\n",
            "{'loss': 0.9202, 'learning_rate': 8.494077834179357e-06, 'epoch': 25.89}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9872345328330994, 'eval_bleu': 52.7325, 'eval_gen_len': 9.1474, 'eval_runtime': 93.3413, 'eval_samples_per_second': 84.411, 'eval_steps_per_second': 5.282, 'epoch': 26.0}\n",
            "{'loss': 0.903, 'learning_rate': 8.38127467569092e-06, 'epoch': 26.14}\n",
            "{'loss': 0.9107, 'learning_rate': 8.268471517202483e-06, 'epoch': 26.4}\n",
            "{'loss': 0.9056, 'learning_rate': 8.155668358714045e-06, 'epoch': 26.65}\n",
            "{'loss': 0.909, 'learning_rate': 8.042865200225607e-06, 'epoch': 26.9}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9888466596603394, 'eval_bleu': 52.8362, 'eval_gen_len': 9.1288, 'eval_runtime': 94.323, 'eval_samples_per_second': 83.532, 'eval_steps_per_second': 5.227, 'epoch': 27.0}\n",
            "{'loss': 0.8963, 'learning_rate': 7.930062041737169e-06, 'epoch': 27.16}\n",
            "{'loss': 0.902, 'learning_rate': 7.817258883248731e-06, 'epoch': 27.41}\n",
            "{'loss': 0.9114, 'learning_rate': 7.704455724760293e-06, 'epoch': 27.66}\n",
            "{'loss': 0.9068, 'learning_rate': 7.591652566271857e-06, 'epoch': 27.92}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9844542741775513, 'eval_bleu': 52.8519, 'eval_gen_len': 9.143, 'eval_runtime': 94.5661, 'eval_samples_per_second': 83.317, 'eval_steps_per_second': 5.213, 'epoch': 28.0}\n",
            "{'loss': 0.9099, 'learning_rate': 7.478849407783419e-06, 'epoch': 28.17}\n",
            "{'loss': 0.9066, 'learning_rate': 7.366046249294982e-06, 'epoch': 28.43}\n",
            "{'loss': 0.8832, 'learning_rate': 7.253243090806544e-06, 'epoch': 28.68}\n",
            "{'loss': 0.8954, 'learning_rate': 7.140439932318105e-06, 'epoch': 28.93}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9850858449935913, 'eval_bleu': 52.8355, 'eval_gen_len': 9.1364, 'eval_runtime': 93.8843, 'eval_samples_per_second': 83.922, 'eval_steps_per_second': 5.251, 'epoch': 29.0}\n",
            "{'loss': 0.9065, 'learning_rate': 7.027636773829668e-06, 'epoch': 29.19}\n",
            "{'loss': 0.9048, 'learning_rate': 6.91483361534123e-06, 'epoch': 29.44}\n",
            "{'loss': 0.8813, 'learning_rate': 6.8020304568527926e-06, 'epoch': 29.7}\n",
            "{'loss': 0.8968, 'learning_rate': 6.689227298364355e-06, 'epoch': 29.95}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9861240983009338, 'eval_bleu': 52.898, 'eval_gen_len': 9.1218, 'eval_runtime': 94.9399, 'eval_samples_per_second': 82.989, 'eval_steps_per_second': 5.193, 'epoch': 30.0}\n",
            "{'loss': 0.8927, 'learning_rate': 6.576424139875917e-06, 'epoch': 30.2}\n",
            "{'loss': 0.879, 'learning_rate': 6.4636209813874795e-06, 'epoch': 30.46}\n",
            "{'loss': 0.8881, 'learning_rate': 6.3508178228990415e-06, 'epoch': 30.71}\n",
            "{'loss': 0.9007, 'learning_rate': 6.238014664410604e-06, 'epoch': 30.96}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.984201967716217, 'eval_bleu': 52.9411, 'eval_gen_len': 9.1245, 'eval_runtime': 94.9137, 'eval_samples_per_second': 83.012, 'eval_steps_per_second': 5.194, 'epoch': 31.0}\n",
            "{'loss': 0.8961, 'learning_rate': 6.125211505922166e-06, 'epoch': 31.22}\n",
            "{'loss': 0.8877, 'learning_rate': 6.012408347433729e-06, 'epoch': 31.47}\n",
            "{'loss': 0.8804, 'learning_rate': 5.899605188945291e-06, 'epoch': 31.73}\n",
            "{'loss': 0.8887, 'learning_rate': 5.7868020304568525e-06, 'epoch': 31.98}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9836801290512085, 'eval_bleu': 52.9578, 'eval_gen_len': 9.1268, 'eval_runtime': 94.5196, 'eval_samples_per_second': 83.358, 'eval_steps_per_second': 5.216, 'epoch': 32.0}\n",
            "{'loss': 0.8801, 'learning_rate': 5.673998871968416e-06, 'epoch': 32.23}\n",
            "{'loss': 0.8981, 'learning_rate': 5.561195713479977e-06, 'epoch': 32.49}\n",
            "{'loss': 0.8864, 'learning_rate': 5.44839255499154e-06, 'epoch': 32.74}\n",
            "{'loss': 0.8798, 'learning_rate': 5.335589396503102e-06, 'epoch': 32.99}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9825403690338135, 'eval_bleu': 52.965, 'eval_gen_len': 9.1236, 'eval_runtime': 93.3036, 'eval_samples_per_second': 84.445, 'eval_steps_per_second': 5.284, 'epoch': 33.0}\n",
            "{'loss': 0.895, 'learning_rate': 5.222786238014664e-06, 'epoch': 33.25}\n",
            "{'loss': 0.8768, 'learning_rate': 5.109983079526227e-06, 'epoch': 33.5}\n",
            "{'loss': 0.8759, 'learning_rate': 4.99717992103779e-06, 'epoch': 33.76}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9835580587387085, 'eval_bleu': 52.9569, 'eval_gen_len': 9.1158, 'eval_runtime': 92.5563, 'eval_samples_per_second': 85.127, 'eval_steps_per_second': 5.326, 'epoch': 34.0}\n",
            "{'loss': 0.8847, 'learning_rate': 4.884376762549351e-06, 'epoch': 34.01}\n",
            "{'loss': 0.8895, 'learning_rate': 4.771573604060914e-06, 'epoch': 34.26}\n",
            "{'loss': 0.8789, 'learning_rate': 4.658770445572476e-06, 'epoch': 34.52}\n",
            "{'loss': 0.8735, 'learning_rate': 4.545967287084039e-06, 'epoch': 34.77}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9822344183921814, 'eval_bleu': 53.0061, 'eval_gen_len': 9.1213, 'eval_runtime': 92.6205, 'eval_samples_per_second': 85.068, 'eval_steps_per_second': 5.323, 'epoch': 35.0}\n",
            "{'loss': 0.8718, 'learning_rate': 4.433164128595601e-06, 'epoch': 35.03}\n",
            "{'loss': 0.8731, 'learning_rate': 4.320360970107164e-06, 'epoch': 35.28}\n",
            "{'loss': 0.8857, 'learning_rate': 4.207557811618725e-06, 'epoch': 35.53}\n",
            "{'loss': 0.8789, 'learning_rate': 4.094754653130288e-06, 'epoch': 35.79}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9818668365478516, 'eval_bleu': 52.9864, 'eval_gen_len': 9.1217, 'eval_runtime': 95.5498, 'eval_samples_per_second': 82.46, 'eval_steps_per_second': 5.16, 'epoch': 36.0}\n",
            "{'loss': 0.8686, 'learning_rate': 3.98195149464185e-06, 'epoch': 36.04}\n",
            "{'loss': 0.8904, 'learning_rate': 3.869148336153413e-06, 'epoch': 36.29}\n",
            "{'loss': 0.8674, 'learning_rate': 3.756345177664975e-06, 'epoch': 36.55}\n",
            "{'loss': 0.8718, 'learning_rate': 3.6435420191765373e-06, 'epoch': 36.8}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9819443821907043, 'eval_bleu': 53.0271, 'eval_gen_len': 9.1184, 'eval_runtime': 98.2126, 'eval_samples_per_second': 80.224, 'eval_steps_per_second': 5.02, 'epoch': 37.0}\n",
            "{'loss': 0.8715, 'learning_rate': 3.5307388606880994e-06, 'epoch': 37.06}\n",
            "{'loss': 0.8802, 'learning_rate': 3.417935702199662e-06, 'epoch': 37.31}\n",
            "{'loss': 0.8709, 'learning_rate': 3.3051325437112243e-06, 'epoch': 37.56}\n",
            "{'loss': 0.8636, 'learning_rate': 3.1923293852227867e-06, 'epoch': 37.82}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9814837574958801, 'eval_bleu': 53.0856, 'eval_gen_len': 9.1263, 'eval_runtime': 94.3238, 'eval_samples_per_second': 83.531, 'eval_steps_per_second': 5.227, 'epoch': 38.0}\n",
            "{'loss': 0.8796, 'learning_rate': 3.079526226734349e-06, 'epoch': 38.07}\n",
            "{'loss': 0.8632, 'learning_rate': 2.966723068245911e-06, 'epoch': 38.32}\n",
            "{'loss': 0.875, 'learning_rate': 2.853919909757473e-06, 'epoch': 38.58}\n",
            "{'loss': 0.8639, 'learning_rate': 2.7411167512690357e-06, 'epoch': 38.83}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9817919135093689, 'eval_bleu': 53.0889, 'eval_gen_len': 9.1098, 'eval_runtime': 92.8713, 'eval_samples_per_second': 84.838, 'eval_steps_per_second': 5.308, 'epoch': 39.0}\n",
            "{'loss': 0.8834, 'learning_rate': 2.628313592780598e-06, 'epoch': 39.09}\n",
            "{'loss': 0.8624, 'learning_rate': 2.5155104342921606e-06, 'epoch': 39.34}\n",
            "{'loss': 0.8733, 'learning_rate': 2.4027072758037226e-06, 'epoch': 39.59}\n",
            "{'loss': 0.8644, 'learning_rate': 2.289904117315285e-06, 'epoch': 39.85}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9820626974105835, 'eval_bleu': 53.052, 'eval_gen_len': 9.1031, 'eval_runtime': 92.6795, 'eval_samples_per_second': 85.013, 'eval_steps_per_second': 5.319, 'epoch': 40.0}\n",
            "{'loss': 0.8631, 'learning_rate': 2.1771009588268475e-06, 'epoch': 40.1}\n",
            "{'loss': 0.8723, 'learning_rate': 2.0642978003384095e-06, 'epoch': 40.36}\n",
            "{'loss': 0.8714, 'learning_rate': 1.951494641849972e-06, 'epoch': 40.61}\n",
            "{'loss': 0.8704, 'learning_rate': 1.8386914833615344e-06, 'epoch': 40.86}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9818735718727112, 'eval_bleu': 53.0611, 'eval_gen_len': 9.1179, 'eval_runtime': 93.2086, 'eval_samples_per_second': 84.531, 'eval_steps_per_second': 5.289, 'epoch': 41.0}\n",
            "{'loss': 0.853, 'learning_rate': 1.7258883248730964e-06, 'epoch': 41.12}\n",
            "{'loss': 0.8658, 'learning_rate': 1.6130851663846589e-06, 'epoch': 41.37}\n",
            "{'loss': 0.8657, 'learning_rate': 1.5002820078962213e-06, 'epoch': 41.62}\n",
            "{'loss': 0.8746, 'learning_rate': 1.3874788494077834e-06, 'epoch': 41.88}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9811019897460938, 'eval_bleu': 53.0419, 'eval_gen_len': 9.1201, 'eval_runtime': 92.1496, 'eval_samples_per_second': 85.502, 'eval_steps_per_second': 5.35, 'epoch': 42.0}\n",
            "{'loss': 0.8607, 'learning_rate': 1.2746756909193458e-06, 'epoch': 42.13}\n",
            "{'loss': 0.8624, 'learning_rate': 1.161872532430908e-06, 'epoch': 42.39}\n",
            "{'loss': 0.8639, 'learning_rate': 1.0490693739424705e-06, 'epoch': 42.64}\n",
            "{'loss': 0.8838, 'learning_rate': 9.362662154540328e-07, 'epoch': 42.89}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9815128445625305, 'eval_bleu': 53.041, 'eval_gen_len': 9.1201, 'eval_runtime': 92.9634, 'eval_samples_per_second': 84.754, 'eval_steps_per_second': 5.303, 'epoch': 43.0}\n",
            "{'loss': 0.8635, 'learning_rate': 8.234630569655951e-07, 'epoch': 43.15}\n",
            "{'loss': 0.8619, 'learning_rate': 7.106598984771574e-07, 'epoch': 43.4}\n",
            "{'loss': 0.8742, 'learning_rate': 5.978567399887198e-07, 'epoch': 43.65}\n",
            "{'loss': 0.862, 'learning_rate': 4.850535815002821e-07, 'epoch': 43.91}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.9813880324363708, 'eval_bleu': 53.0484, 'eval_gen_len': 9.1182, 'eval_runtime': 93.3288, 'eval_samples_per_second': 84.422, 'eval_steps_per_second': 5.282, 'epoch': 44.0}\n",
            "{'loss': 0.8626, 'learning_rate': 3.7225042301184434e-07, 'epoch': 44.16}\n",
            "{'loss': 0.8644, 'learning_rate': 2.594472645234067e-07, 'epoch': 44.42}\n",
            "{'loss': 0.8813, 'learning_rate': 1.4664410603496897e-07, 'epoch': 44.67}\n",
            "{'loss': 0.8648, 'learning_rate': 3.3840947546531305e-08, 'epoch': 44.92}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "{'eval_loss': 0.981381356716156, 'eval_bleu': 53.0595, 'eval_gen_len': 9.1187, 'eval_runtime': 94.2418, 'eval_samples_per_second': 83.604, 'eval_steps_per_second': 5.231, 'epoch': 45.0}\n",
            "{'train_runtime': 11917.2073, 'train_samples_per_second': 118.995, 'train_steps_per_second': 7.439, 'train_loss': 0.9788824253760013, 'epoch': 45.0}\n"
          ]
        }
      ],
      "source": [
        "!python ../src/models/train_model.py --train_path={root}data/interim/train.csv --val_path={root}data/interim/val.csv --save_dir={root}/models/output_dir/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result:\n",
        "BLEU score on validation dataset is **53%**.\n",
        "\n",
        "That is even higher than simple swearing removal on 8%. Meaning the model tries to paraphrase almost completing the task as it was required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SJ70RMtLdj"
      },
      "source": [
        "# Test fine tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HpaXA8UyXjW"
      },
      "source": [
        "### Comment one of 3 given texts or write yours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-01 14:18:57.748045: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-01 14:18:57.748111: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-01 14:18:57.748154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-01 14:18:57.755917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspaces/PMLDL_Assignment1/src\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: they invade france and they murder white people | Input: fucking niggers invade france and murder white gays\n",
            "Translation: oh my big and beautiful pet | Input: suck my big and beautiful dick\n",
            "Translation: it didn t mess with him | Input: it didn t fuck him\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "module_path = f'{project_path}/src/models/'\n",
        "sys.path.append(module_path)  \n",
        "\n",
        "from predict_model import predict\n",
        "\n",
        "texts =  [\"Fucking niggers invade France and murder white gays\", \n",
        "          \"Suck my big, and beautiful dick.\", \n",
        "          \"It didn't fuck him.\"]\n",
        "\n",
        "predict(root, texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiUa833xtOJv"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tdRd42nOtPRA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import SVG, display, HTML\n",
        "def svg(source, title):\n",
        "    display(HTML(f'<h3>{title}</h3><img src=\"' + source + '\" width=500 height=350/>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3>BLEU score on validation dataset</h3><img src=\"../reports/figures/training_plots/eval_bleu.svg\" width=500 height=350/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Validation loss</h3><img src=\"../reports/figures/training_plots/eval_loss.svg\" width=500 height=350/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Training loss</h3><img src=\"../reports/figures/training_plots/train_loss.svg\" width=500 height=350/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Learning rate</h3><img src=\"../reports/figures/training_plots/train_learning_rate.svg\" width=500 height=350/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "svg(f'{root}reports/figures/training_plots/eval_bleu.svg', 'BLEU score on validation dataset')\n",
        "svg(f'{root}reports/figures/training_plots/eval_loss.svg', 'Validation loss')\n",
        "svg(f'{root}reports/figures/training_plots/train_loss.svg', 'Training loss')\n",
        "svg(f'{root}reports/figures/training_plots/train_learning_rate.svg', 'Learning rate')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
